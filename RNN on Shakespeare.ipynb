{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trains a neural network using RNN cells on shakespeare \n",
    "#Training example found here:\n",
    "#https://cs.stanford.edu/people/karpathy/char-rnn/shakespear.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape the website for data\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = '''https://cs.stanford.edu/people/karpathy/char-rnn/shakespear.txt'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get(link).text\n",
    "soup = BeautifulSoup(html,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That, poor contempt, or claim'd thou slept so fait\n",
      "HAM:\n",
      "You guess: I'll take my bloody back.\n",
      "\n",
      "BRUTUS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = soup.find_all(text = True)[0]\n",
    "print(text[0:50])\n",
    "print(text[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "#Begin making the neural network\n",
    "import sys,random,math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import sys\n",
    "%run Tensor_Framework_with_Numpy.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "vocab = list(set(text)) #Get every unique word in Shakespeare\n",
    "word2index = { } #Get indices for each word\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i #Build indices\n",
    "indices = np.array(list(map(lambda x:word2index[x],text))) #Get the indices for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60 55 16 20  8]\n",
      "{'d': 0, ';': 1, 'E': 2, '.': 3, \"'\": 4, 'w': 5, 'H': 6, 'C': 7, ',': 8, 'n': 9, 'O': 10, 'q': 11, 'x': 12, 's': 13, '\\n': 14, ':': 15, 'a': 16, 'j': 17, 'e': 18, 'I': 19, 't': 20, 'b': 21, 'u': 22, 'o': 23, 'k': 24, 'P': 25, 'p': 26, 'Q': 27, 'Y': 28, 'X': 29, 'D': 30, 'B': 31, 'y': 32, 'f': 33, 'M': 34, '!': 35, 'N': 36, 'i': 37, 'r': 38, 'm': 39, 'J': 40, 'l': 41, '-': 42, 'Z': 43, '?': 44, 'c': 45, 'F': 46, 'A': 47, 'U': 48, 'z': 49, 'g': 50, ' ': 51, 'G': 52, 'R': 53, 'W': 54, 'h': 55, 'K': 56, 'v': 57, 'V': 58, 'L': 59, 'T': 60, 'S': 61}\n",
      "['d', ';', 'E', '.', \"'\"]\n"
     ]
    }
   ],
   "source": [
    "print(indices[0:5])\n",
    "print(word2index)\n",
    "print(vocab[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedding(vocab_size = len(vocab), dim = 512)\n",
    "model = RNNCell(n_inputs = 512, n_hidden = 512, n_output = len(vocab))\n",
    "criterion = CrossEntropyLoss()\n",
    "optim = SGD(parameters = model.get_parameters() + embed.get_parameters(),\n",
    "           alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 #Train 32 records at a time\n",
    "bptt = 16 #Specify backpropagation to stop 16 steps into the past\n",
    "n_batches = int((indices.shape[0] / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Because backprop is truncated, we need to subset datasets of size bptt\n",
    "trimmed_indices = indices[:n_batches * batch_size]\n",
    "batched_indices = trimmed_indices.reshape(batch_size, n_batches).transpose()\n",
    "\n",
    "input_batched_indices = batched_indices[0:-1] #Subset the data to create predicted and target data\n",
    "target_batched_indices = batched_indices[1:]\n",
    "\n",
    "n_bptt = int(((n_batches - 1) / bptt)) #Number of backprops to perform\n",
    "input_batches = input_batched_indices[:n_bptt * bptt] #Of input indices, create batches\n",
    "input_batches = input_batches.reshape(n_bptt,bptt,batch_size) #Reshape for input\n",
    "target_batches = target_batched_indices[:n_bptt * bptt] #Create final target batches\n",
    "target_batches = target_batches.reshape(n_bptt,bptt,batch_size) #Reshape the same size as input batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99968,)\n",
      "(3124, 32)\n",
      "(3123, 32)\n",
      "(3123, 32)\n",
      "(195, 16, 32)\n",
      "(195, 16, 32)\n",
      "195\n",
      "\n",
      "That,\n",
      "[60 55 16 20  8]\n",
      "[60 55 16 20  8]\n",
      "\n",
      "[60 55 16 20  8]\n",
      "[55 16 20  8 51]\n"
     ]
    }
   ],
   "source": [
    "#Aside for readability\n",
    "print(trimmed_indices.shape)\n",
    "print(batched_indices.shape)\n",
    "print(input_batched_indices.shape)\n",
    "print(target_batched_indices.shape)\n",
    "print(input_batches.shape)\n",
    "print(target_batches.shape)\n",
    "print(n_bptt)\n",
    "print()\n",
    "print(text[0:5])\n",
    "print(indices[0:5])\n",
    "print(batched_indices[0:5,0]) \n",
    "print()\n",
    "print(input_batches[0][0:5,0])\n",
    "print(target_batches[0][0:5,0])#Note the target batch is the input batch offset by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(output = None,n = 30, init_char = ' '):\n",
    "    s = ''\n",
    "    hidden = model.init_hidden(batch_size = 1)\n",
    "    input = Tensor(np.array([word2index[init_char]]))\n",
    "    for i in range(n):\n",
    "        rnn_input = embed.forward(input)\n",
    "        output.hidden = model.forward(input = rnn_input, hidden = hidden)\n",
    "        output.data *= 10\n",
    "        temp_dest = output.cross_entropy(target_batched_indices)\n",
    "        temp_dist /= temp_dist.sum()\n",
    "        \n",
    "        m = (temp_dist > np.random.rand()).argmax()\n",
    "        c = vocab[m]\n",
    "        input = Tensor(np.array([m]))\n",
    "        s += c\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network training function\n",
    "def train(iterations = 100):\n",
    "    for iter in range(iterations):\n",
    "        total_loss = 0 #Start loss measure\n",
    "        n_loss = 0\n",
    "        \n",
    "        hidden = model.init_hidden(batch_size = batch_size)\n",
    "        for batch_i in range(len(input_batches)):\n",
    "            hidden = Tensor(hidden.data, autograd = True)#Create hidden layer\n",
    "            loss = None\n",
    "            losses = list()\n",
    "            for t in range(bptt): #Start backprop tensors\n",
    "                input = Tensor(input_batches[batch_i][t],autograd = True)\n",
    "                rnn_input = embed.forward(input = input) #Embedding layer for forward prop\n",
    "                output, hidden = model.forward(input = rnn_input,\n",
    "                                              hidden = hidden)\n",
    "                target = Tensor(target_batches[batch_i][t], autograd = True) #Define target\n",
    "                batch_loss = criterion.forward(output,target) #Measure loss\n",
    "                losses.append(batch_loss)\n",
    "                if(t == 0):\n",
    "                    loss = batch_loss #Define first loss, else increase the loss\n",
    "                else:\n",
    "                    loss += batch_loss\n",
    "            loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "            optim.step()\n",
    "            total_loss += loss.data\n",
    "        optim.alpha *= 0.99\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_sample(n=2000, init_char='\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
